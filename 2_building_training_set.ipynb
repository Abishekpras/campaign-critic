{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering meta features and building a training set \n",
    "\n",
    "**Goal: Load a collection of scraped content from Kickstarter project pages, parse the HTML, extract both campaign sections and normalize the text. Next, perform feature engineering and prepare a training set that includes the target variable. The completed training sets are saved in separate tables for each campaign section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the functions for parsing, extracting and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(scraped_html):\n",
    "    \"\"\"Use the BeautifulSoup library to parse the scraped HTML of a project \n",
    "    using an lxml parser\n",
    "    \n",
    "    Args:\n",
    "        scraped_html (response object): the unparsed response object collected\n",
    "        by the web scraper\n",
    "    \n",
    "    Returns:\n",
    "        a soup object containing the parsed HTML\"\"\"\n",
    "\n",
    "    # Parse the HTML content using an lxml parser\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    \"\"\"Clean up the text of a campaign section by removing unnecessary and\n",
    "    extraneous content\n",
    "    \n",
    "    Args:\n",
    "        messy_text (str): the raw text from a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a string containing the cleaned text\"\"\"\n",
    "    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compress all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    \"\"\"Extract the two campaign sections, \"About this project\" and \"Risk and\n",
    "    challenges\", of a Kickstarter project\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "    \n",
    "    Returns:\n",
    "        a dictionary of 2 strings containing each campaign section\"\"\"\n",
    "    \n",
    "    # Collect the \"About this project\" section if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Collect the \"Risks and challenges\" section if available, and remove all\n",
    "    # unnecessary text\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean both campaign sections\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Tag meta content inside a campaign section, such as email addresses,\n",
    "    hyperlinks, money amounts, percentages, phone numbers, and numbers to\n",
    "    avoid adding these to the word count\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a string containing the text of a campaign section with all the meta\n",
    "        content tagged\"\"\"\n",
    "    \n",
    "    # Tag email addresses with regex\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks with regex\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag money amounts with regex\n",
    "    normalized = re.sub(r'\\$\\d+(\\.\\d+)?', 'dollramt', normalized)\n",
    "    \n",
    "    # Tag percentages with regex\n",
    "    normalized = re.sub(r'\\d+(\\.\\d+)?\\%', 'percntg', normalized)\n",
    "    \n",
    "    # Tag phone numbers with regex\n",
    "    normalized = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag remaining numbers with regex\n",
    "    return re.sub(r'\\d+(\\.\\d+)?', 'numbr', normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the feature extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"Use the sentence tokenizer from the nltk library to extract sentences \n",
    "    from the text of a campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a list containing every sentence of a campaign section\"\"\"\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    \"\"\"Remove all punctuation from the text of a campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a string containing the text of a campaign section without any\n",
    "        punctuation\"\"\"\n",
    "    \n",
    "    # Remove punctuation with regex\n",
    "    return re.sub(r'[^\\w\\d\\s]|\\_', ' ', text)\n",
    "\n",
    "def get_words(text):\n",
    "    \"\"\"Use the word tokenizer from the nltk library to extract words from the \n",
    "    text of a campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a list containing every word of a campaign section\"\"\"\n",
    "    \n",
    "    # Remove punctuation and then tokenize the text into words\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text))]\n",
    "\n",
    "def identify_allcaps(text):\n",
    "    \"\"\"Find all examples where a word is written in all capital letters in the \n",
    "    text of a campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a list containing every all-caps word of a campaign section\"\"\"\n",
    "        \n",
    "    # Identify all-caps words with regex\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)\n",
    "\n",
    "def count_exclamations(text):\n",
    "    \"\"\"Count the number of exclamation marks in the text of a campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of exclamation marks in the text\"\"\"\n",
    "    \n",
    "    # Count the number of exclamation marks in the text\n",
    "    return text.count('!')\n",
    "\n",
    "def count_apple_words(text):\n",
    "    \"\"\"Count the number of innovation-related words (called Apple words) in the\n",
    "    text of a campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of Apple words in the text\"\"\"\n",
    "    \n",
    "    # Define a set of adjectives used commonly by Apple marketing team\n",
    "    # according to https://www.youtube.com/watch?v=ZWPqjXYTqYw\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    # Count total number of Apple words in the text\n",
    "    return sum(1 for word in get_words(text) if word in apple_words)\n",
    "\n",
    "def compute_avg_words(text):\n",
    "    \"\"\"Count the average number of words in each sentence in the text of a \n",
    "    campaign section\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a float representing the average number of words in each sentence of\n",
    "        the text\"\"\"\n",
    "    \n",
    "    # Compute the average number of words in each sentence\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in get_sentences(text)]\n",
    "    ).mean()\n",
    "\n",
    "def count_paragraphs(soup, section):    \n",
    "    \"\"\"Count the number of paragraph tags in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of paragraphs in the campaign\n",
    "        section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to count the number of paragraphs depending on which\n",
    "    # section is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p'))\n",
    "    \n",
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    \"\"\"Count the average number of sentences per paragraph in a campaign \n",
    "    section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        a float representing the average number of sentences in each paragraph\n",
    "        of a campaign section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to identify all paragraphs depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of sentences in each paragraph    \n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def compute_avg_words_paragraph(soup, section):\n",
    "    \"\"\"Count the average number of words per paragraph in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        a float representing the average number of words in each paragraph\n",
    "        of a campaign section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to identify all paragraphs depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of words in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def count_images(soup, section):    \n",
    "    \"\"\"Count the number of image tags in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of images in a campaign section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to identify all image tags depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img'))\n",
    "    \n",
    "def count_videos(soup, section):    \n",
    "    \"\"\"Count the number of non-YouTube videos in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of non-YouTube videos in a campaign\n",
    "        section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to count all non-YouTube video tags depending on which\n",
    "    # section is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "         ).find_all('div', class_='video-player'))\n",
    "\n",
    "def count_youtube(soup, section):    \n",
    "    \"\"\"Count the number of YouTube videos in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of YouTube videos in a campaign\n",
    "        section\"\"\"\n",
    "    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to identify all iframe tags depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    # Since YouTube videos are contained only in iframe tags, determine which\n",
    "    # iframe tags contain YouTube videos and count them\n",
    "    for iframe in iframes:\n",
    "        # Catch any iframes that fail to include a YouTube source link\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count\n",
    "\n",
    "def count_gifs(soup, section):    \n",
    "    \"\"\"Count the number of GIF images in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of GIF images in a campaign\n",
    "        section\"\"\"\n",
    "    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to select all image tags depending on the section\n",
    "    # requested\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    # Since GIFs are contained in image tags, determine which image tags\n",
    "    # contain GIFs and count them\n",
    "    for image in images:\n",
    "        # Catch any iframes that fail to include an image source link\n",
    "        try: \n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count\n",
    "\n",
    "def count_hyperlinks(soup, section):    \n",
    "    \"\"\"Count the number of hyperlink tags in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of hyperlinks in a campaign\n",
    "        section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to compute number of hyperlink tags depending on the\n",
    "    # section requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('a'))\n",
    "    \n",
    "def count_bolded(soup, section):    \n",
    "    \"\"\"Count the number of bold tags in a campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        an integer representing the number of bolded text tags in a campaign\n",
    "        section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to compute number of bolded text tags depending on which\n",
    "    # section is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('b'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Perform text preprocessing such as removing punctuation, lowercasing all\n",
    "    words, removing stop words and stemming remaining words\n",
    "    \n",
    "    Args:\n",
    "        text (str): cleaned and normalized text of a campaign section\n",
    "    \n",
    "    Returns:\n",
    "        a string containing text that has been preprocessed\"\"\"\n",
    "    \n",
    "    # Access stop word dictionary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # Initialize the Porter stemmer\n",
    "    porter = nltk.PorterStemmer()\n",
    "    \n",
    "    # Remove punctuation and lowercase each word\n",
    "    text = remove_punc(text).lower()\n",
    "    \n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join(\n",
    "        porter.stem(term )\n",
    "        for term in text.split()\n",
    "        if term not in set(stop_words)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the table containing a collection of scraped HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load scraped data\n",
    "filename = 'data/scraped/scraped_collection_16692-19999.pkl'\n",
    "scraped_collection = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare an aggregate function that extracts all meta features for a campaign section of a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(soup, campaign, section):\n",
    "    \"\"\"Extract the meta features of the text of campaign section\n",
    "    \n",
    "    Args:\n",
    "        soup (soup object): parsed HTML content of a Kickstarter project page\n",
    "        campaign (dict): dictionary of strings containing both campaign\n",
    "            sections\n",
    "        section (str): label of the campaign section to be analyzed\n",
    "    \n",
    "    Returns:\n",
    "        a tuple containing 19 extracted meta features and a string containing\n",
    "        the normalized text of the campaign section, otherwise a list of 20\n",
    "        NaN values if the campaign section does not exist\"\"\"\n",
    "    \n",
    "    # Compute the number of words in the requested section\n",
    "    num_words = len(get_words(campaign[section]))\n",
    "    \n",
    "    # If the section contains no words, assign NaN to 'num_words' to avoid\n",
    "    # potential division by zero\n",
    "    if num_words == 0:\n",
    "        num_words = np.nan\n",
    "        \n",
    "    # Extract all meta features for the requested section in addition to the\n",
    "    # normalized campaign text. If the section isn't available, then return \n",
    "    # NaN for each meta feature.\n",
    "    if campaign[section] == 'section_not_found':\n",
    "        return([np.nan] * 20)\n",
    "    else:\n",
    "        return (\n",
    "            len(get_sentences(campaign[section])),\n",
    "            num_words,\n",
    "            len(identify_allcaps(campaign[section])),\n",
    "            len(identify_allcaps(campaign[section])) / num_words,\n",
    "            count_exclamations(campaign[section]),\n",
    "            count_exclamations(campaign[section]) / num_words,\n",
    "            count_apple_words(campaign[section]),\n",
    "            count_apple_words(campaign[section]) / num_words,\n",
    "            compute_avg_words(campaign[section]),\n",
    "            count_paragraphs(soup, section),\n",
    "            compute_avg_sents_paragraph(soup, section),\n",
    "            compute_avg_words_paragraph(soup, section),\n",
    "            count_images(soup, section),\n",
    "            count_videos(soup, section),\n",
    "            count_youtube(soup, section),\n",
    "            count_gifs(soup, section),\n",
    "            count_hyperlinks(soup, section),\n",
    "            count_bolded(soup, section),\n",
    "            count_bolded(soup, section) / num_words,\n",
    "            campaign[section]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize empty DataFrames to store meta features and the normalized text of each campaign section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames of features for each section\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "            'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "            'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "            'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "            'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "            'num_hyperlinks', 'num_bolded', 'percent_bolded',\n",
    "            'normalized_text']\n",
    "section1_df = pd.DataFrame(columns=features)\n",
    "section2_df = pd.DataFrame(columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's parse through the scraped HTML for each project, pull out the campaign sections and clean them, normalize the text, and extract meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in scraped_collection.iterrows():\n",
    "    # Parse scraped HTML\n",
    "    soup = parse(row[0])\n",
    "    \n",
    "    # Extract and normalize campaign sections\n",
    "    campaign = get_campaign(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    campaign['risks'] = normalize(campaign['risks'])\n",
    "    \n",
    "    # Extract meta features for each section\n",
    "    section1_df.loc[index] = extract_features(soup, campaign, 'about')\n",
    "    section2_df.loc[index] = extract_features(soup, campaign, 'risks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining extracted meta features with Web Robots data\n",
    "\n",
    "Since the Web Robots data contains the target variable, in addition to other interesting features, let's join these data with the extracted meta features and normalized text to complete the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Web Robots data\n",
    "web_robots_data = joblib.load(\n",
    "    'data/web_robots_data/web_robots_data_to_06-2017.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a shared key to complete the join. Let's turn the index labels into a new column called `index`&mdash;it'll serve as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn the index labels into a new column\n",
    "web_robots_data = web_robots_data.reset_index()\n",
    "section1_df = section1_df.reset_index()\n",
    "section2_df = section2_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join the extracted meta features and normalized text with the Web Robots data, containing the target variable, for each campaign section using only the projects whose features have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join Web Robots data with extracted features for each section\n",
    "section1_merged = section1_df.merge(web_robots_data, how='left', on='index')\n",
    "section2_merged = section2_df.merge(web_robots_data, how='left', on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the joined data, let's extract the starting and ending points to label the filename with the data it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the starting and ending projects using the filename of the database\n",
    "# containing the scraped HTML collection\n",
    "starting_point = int(filename.partition('collection_')[2].partition('-')[0])\n",
    "ending_point = int(filename.partition('-')[2].partition('.pkl')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the table containing the training set for each section and label their filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section2_all_features_16692-19999.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the data table for each section\n",
    "joblib.dump(section1_merged, 'section1_all_features_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point\n",
    "    )\n",
    ")\n",
    "\n",
    "joblib.dump(section2_merged, 'section2_all_features_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
