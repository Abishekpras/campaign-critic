{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering meta features and building a training set \n",
    "\n",
    "**Goal: Load a collection of scraped content from Kickstarter project pages, parse the HTML, extract both campaign sections and normalize the text. Next, perform meta feature engineering and prepares a training set, including target variable, all of which are saved in separate tables for each campaign section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the functions for parsing, extracting and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(scraped_html):\n",
    "    # Parse the HTML content using an lxml parser\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compress all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    # Collect the \"About this project\" section if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Collect the \"Risks and challenges\" section if available, and remove #\n",
    "    # unnecessary text\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean both sections and return them in a dictionary\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}\n",
    "\n",
    "def normalize(text):\n",
    "    # Tag email addresses\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag money amounts\n",
    "    normalized = re.sub(r'\\$\\d+(\\.\\d+)?', 'dollramt', normalized)\n",
    "    \n",
    "    # Tag percentages\n",
    "    normalized = re.sub(r'\\d+(\\.\\d+)?\\%', 'percntg', normalized)\n",
    "    \n",
    "    # Tag phone numbers\n",
    "    normalized = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag remaining numbers\n",
    "    return re.sub(r'\\d+(\\.\\d+)?', 'numbr', normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the feature extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    # Tokenize text into sentences and return them in a list\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    # Return the text with punctuation removed\n",
    "    return re.sub(r'[^\\w\\d\\s]|\\_', ' ', text)\n",
    "\n",
    "def get_words(text):\n",
    "    # Tokenize text into words and return them in a list\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text))]\n",
    "\n",
    "def identify_allcaps(text):\n",
    "    # Count the number of all-caps words\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)\n",
    "\n",
    "def count_exclamations(text):\n",
    "    # Count the number of exclamation marks in the text\n",
    "    return text.count('!')\n",
    "\n",
    "def count_apple_words(text):\n",
    "    # Define a set of Apple adjectives\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    # Count total number of Apple adjectives in the text\n",
    "    return sum(1 for word in get_words(text) if word in apple_words)\n",
    "\n",
    "def compute_avg_words(text):\n",
    "    # Compute the average number of words in each sentence\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in get_sentences(text)]\n",
    "    ).mean()\n",
    "\n",
    "def count_paragraphs(soup, section):    \n",
    "    # Use tree parsing to compute number of paragraphs\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p'))\n",
    "    \n",
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of sentences in each paragraph    \n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def compute_avg_words_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of words in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def count_images(soup, section):    \n",
    "    # Use tree parsing to compute number of images\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img'))\n",
    "    \n",
    "def count_videos(soup, section):    \n",
    "    # Use tree parsing to compute number of non-YouTube videos\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "         ).find_all('div', class_='video-player'))\n",
    "\n",
    "def count_youtube(soup, section):    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to select all iframe tags\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    # Since YouTube videos are contained only in iframe tags, determine which\n",
    "    # iframe tags contain YouTube videos and count them\n",
    "    for iframe in iframes:\n",
    "        # Catch any iframes that fail to include a YouTube source link\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count\n",
    "\n",
    "def count_gifs(soup, section):    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to select all image tags\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    # Since GIFs are contained in image tags, determine which image tags\n",
    "    # contain GIFs and count them\n",
    "    for image in images:\n",
    "        # Catch any iframes that fail to include an image source link\n",
    "        try: \n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count\n",
    "\n",
    "def count_hyperlinks(soup, section):    \n",
    "    # Use tree parsing to compute number of hyperlinks\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('a'))\n",
    "    \n",
    "def count_bolded(soup, section):    \n",
    "    # Use tree parsing to compute number of bolded text tags\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('b'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Access stop word dictionary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # Initialize the Porter stemmer\n",
    "    porter = nltk.PorterStemmer()\n",
    "    \n",
    "    # Remove punctuation and lowercase each word\n",
    "    text = remove_punc(text).lower()\n",
    "    \n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join(\n",
    "        porter.stem(term )\n",
    "        for term in text.split()\n",
    "        if term not in set(stop_words)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the table containing a collection of scraped HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load scraped data\n",
    "filename = 'data/scraped/scraped_collection_16692-19999.pkl'\n",
    "scraped_collection = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare an aggregate function that extracts all meta features for a campaign section of a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(soup, campaign, section):\n",
    "    # Compute the number of words in the section\n",
    "    num_words = len(get_words(campaign[section]))\n",
    "    \n",
    "    # If the section contains no words, assign NaN to 'num_words' to avoid\n",
    "    # potential division by zero\n",
    "    if num_words == 0:\n",
    "        num_words = np.nan\n",
    "        \n",
    "    # Extract all features for the given section. If the section isn't \n",
    "    # available, then return np.nan for each feature.\n",
    "    if campaign[section] == 'section_not_found':\n",
    "        return([np.nan] * 20)\n",
    "    else:\n",
    "        return (\n",
    "            len(get_sentences(campaign[section])),\n",
    "            num_words,\n",
    "            len(identify_allcaps(campaign[section])),\n",
    "            len(identify_allcaps(campaign[section])) / num_words,\n",
    "            count_exclamations(campaign[section]),\n",
    "            count_exclamations(campaign[section]) / num_words,\n",
    "            count_apple_words(campaign[section]),\n",
    "            count_apple_words(campaign[section]) / num_words,\n",
    "            compute_avg_words(campaign[section]),\n",
    "            count_paragraphs(soup, section),\n",
    "            compute_avg_sents_paragraph(soup, section),\n",
    "            compute_avg_words_paragraph(soup, section),\n",
    "            count_images(soup, section),\n",
    "            count_videos(soup, section),\n",
    "            count_youtube(soup, section),\n",
    "            count_gifs(soup, section),\n",
    "            count_hyperlinks(soup, section),\n",
    "            count_bolded(soup, section),\n",
    "            count_bolded(soup, section) / num_words,\n",
    "            campaign[section]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize empty DataFrames for meta features and the normalized text of each campaign section and set the column names with feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames of features for each section\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "            'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "            'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "            'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "            'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "            'num_hyperlinks', 'num_bolded', 'percent_bolded',\n",
    "            'normalized_text']\n",
    "section1_df = pd.DataFrame(columns=features)\n",
    "section2_df = pd.DataFrame(columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's parse through the scraped HTML for each project, pull out the campaign sections and clean them, normalize the text, and extract meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in scraped_collection.iterrows():\n",
    "    # Parse scraped HTML\n",
    "    soup = parse(row[0])\n",
    "    \n",
    "    # Extract and normalize campaign sections\n",
    "    campaign = get_campaign(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    campaign['risks'] = normalize(campaign['risks'])\n",
    "    \n",
    "    # Extract meta features for each section\n",
    "    section1_df.loc[index] = extract_features(soup, campaign, 'about')\n",
    "    section2_df.loc[index] = extract_features(soup, campaign, 'risks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining extracted features with Web Robots data\n",
    "\n",
    "Since the Web Robots data contains the target variable, in addition to other interesting features, let's join these data with the extracted meta features and normalized text to complete the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Web Robots data\n",
    "web_robots_data = joblib.load(\n",
    "    'data/web_robots_data/web_robots_data_to_06-2017.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a shared key to complete the join. Let's turn the index labels into a new column called `index`&mdash;it'll serve as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn the index labels into a new column\n",
    "web_robots_data = web_robots_data.reset_index()\n",
    "section1_df = section1_df.reset_index()\n",
    "section2_df = section2_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join the extracted meta features and normalized text with the Web Robots data, containing the target variable, for each campaign section using only the projects whose features have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join Web Robots data with extracted features for each section\n",
    "section1_merged = section1_df.merge(web_robots_data, how='left', on='index')\n",
    "section2_merged = section2_df.merge(web_robots_data, how='left', on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the joined data, let's extract the starting and ending points to label the filename with the data it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the start and end projects using the filename of the database\n",
    "# containing the scraped HTML collection\n",
    "starting_point = int(filename.partition('collection_')[2].partition('-')[0])\n",
    "ending_point = int(filename.partition('-')[2].partition('.pkl')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the table containing the training set for each section and label their filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section2_all_features_16692-19999.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the data table for each section\n",
    "joblib.dump(section1_merged, 'section1_all_features_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point\n",
    "    )\n",
    ")\n",
    "\n",
    "joblib.dump(section2_merged, 'section2_all_features_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
