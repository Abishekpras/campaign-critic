{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load scraping, parsing, and extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(hyperlink):\n",
    "    # Scrape the website\n",
    "    return requests.get(hyperlink)\n",
    "\n",
    "def parse(scraped_html):\n",
    "    # Parse the HTML\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compresses all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    # Extract the 'About this project' section, if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Extract the 'Risks and challenges' section, if available\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean up both sections and return them in a dict\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"def get_section1(soup):\n",
    "    # Extracts the 'About this project' section\n",
    "    #return text.partition('About this project')[2] \\\n",
    "    #    .partition('Risks and challenges')[0]\n",
    "    try:\n",
    "        return soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        return 'not_found'\n",
    "    \n",
    "def get_section2(soup):\n",
    "    # Extracts the 'Risks and challenges' section\n",
    "    #return text.partition('Risks and challenges')[2] \\\n",
    "    #    .partition('Learn about accountability on Kickstarter')[0]\n",
    "    try:\n",
    "        return soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).get_text(' ').replace('Risks and challenges', '')\n",
    "    except AttributeError:\n",
    "        return 'not_found'\n",
    "\n",
    "def extract_sections(soup):    \n",
    "    # Extract and clean up both sections\n",
    "    #return (\n",
    "    #    clean_up(get_section1(soup.get_text(' '))), \n",
    "    #    clean_up(get_section2(soup.get_text(' ')))\n",
    "    #)\n",
    "    return (\n",
    "        clean_up(get_section1(soup)),\n",
    "        clean_up(get_section2(soup))\n",
    "    )\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape HTML content from a hyperlink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperlink = 'https://www.kickstarter.com/projects/getpebble/pebble-2-time-2-and-core-an-entirely-new-3g-ultra'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/sbf/sculpto-the-worlds-most-user-friendly-desktop-3d-p?ref=discovery'\n",
    "hyperlink = 'https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/1683069409/the-new-york-sorta-marathon?ref=discovery'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/dinobytelabs/midli-a-dark-and-mystical-tale-of-letting-go?ref=category'\n",
    "scraped_html = scrape(hyperlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse HTML content and extract sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = parse(scraped_html)\n",
    "campaign = get_campaign(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize text\n",
    "\n",
    "Define a function to tag specific components of the sections, such as hyperlinks, as to avoid interfering with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Tag email addresses\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign['about'] = normalize(campaign['about'])\n",
    "campaign['risks'] = normalize(campaign['risks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to compute features\n",
    "For each feature, test the function on the *About this project* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    # Tokenizes text into sentences and returns them in a list\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    num_sents = np.nan\n",
    "else:\n",
    "    num_sents = len(get_sentences(campaign['about']))\n",
    "num_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    # Returns the text with punctuation removed\n",
    "    return re.sub(r'[^\\w\\d\\s]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    # Tokenizes text into words and returns them in a list, excluding tags\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text)) \\\n",
    "            if word not in ('emailaddr', 'httpaddr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1295"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    num_words = np.nan\n",
    "else:\n",
    "    num_words = len(get_words(campaign['about']))\n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of all-caps words and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_allcaps(text):\n",
    "    # Counts the number of all-caps words\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 0.02084942084942085\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        len(identify_allcaps(campaign['about'])),\n",
    "        len(identify_allcaps(campaign['about'])) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of exclamation marks and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_exclamations(text):\n",
    "    # Counts the number of exclamation marks present in the text\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.009266409266409266\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_exclamations(campaign['about']),\n",
    "        count_exclamations(campaign['about']) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of Apple adjectives and %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_apple_words(text):\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    return sum(\n",
    "        1 for word in get_words(text) if word in apple_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.0015444015444015444\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_apple_words(campaign['about']),\n",
    "        count_apple_words(campaign['about']) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average # of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_words(text):\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in \\\n",
    "         get_sentences(text)]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4601769912\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_words(campaign['about']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the # of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_paragraphs(soup, section):    \n",
    "    # Use tree parsing to compute # of paragraphs for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_paragraphs(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average # of sentences per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "        \n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.94871794872\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_sents_paragraph(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average # of words per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_words_paragraph(soup, section):\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.4358974359\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_words_paragraph(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOUP: Count # of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_images(soup, section):    \n",
    "    # Use tree parsing to compute # of images for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_images(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of embedded videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_videos(soup, section):    \n",
    "    # Use tree parsing to compute # of images for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('div', class_='video-player'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_videos(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_youtube(soup, section):    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to compute # of YouTube videos for each section\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    for iframe in iframes:\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_youtube(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_gifs(soup, section):    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to compute # of GIFs for each section\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    for image in images:\n",
    "        try:\n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_gifs(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_hyperlinks(soup, section):    \n",
    "    # Use tree parsing to compute hyperlinks for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_hyperlinks(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of bolded text and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_bolded(soup, section):    \n",
    "    # Use tree parsing to compute hyperlinks for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 0.032432432432432434\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_bolded(soup, 'about'),\n",
    "        count_bolded(soup, 'about') / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all features for one project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 1295, 27, 0.02084942084942085, 12, 0.009266409266409266, 2, 0.0015444015444015444, 11.460176991150442, 39, 2.9487179487179489, 29.435897435897434, 11, 0, 0, 0, 15, 42, 0.032432432432432434)\n"
     ]
    }
   ],
   "source": [
    "section = 'about'\n",
    "if campaign[section] == 'section_not_found':\n",
    "    print([np.nan] * 19)\n",
    "else:\n",
    "    row = ( \n",
    "        len(get_sentences(campaign[section])),\n",
    "        len(get_words(campaign[section])),\n",
    "        len(identify_allcaps(campaign[section])),\n",
    "        len(identify_allcaps(campaign[section])) / num_words,\n",
    "        count_exclamations(campaign[section]),\n",
    "        count_exclamations(campaign[section]) / num_words,\n",
    "        count_apple_words(campaign[section]),\n",
    "        count_apple_words(campaign[section]) / num_words,\n",
    "        compute_avg_words(campaign[section]),\n",
    "        count_paragraphs(soup, section),\n",
    "        compute_avg_sents_paragraph(soup, section),\n",
    "        compute_avg_words_paragraph(soup, section),\n",
    "        count_images(soup, section),\n",
    "        count_videos(soup, section),\n",
    "        count_youtube(soup, section),\n",
    "        count_gifs(soup, section),\n",
    "        count_hyperlinks(soup, section),\n",
    "        count_bolded(soup, section),\n",
    "        count_bolded(soup, section) / num_words\n",
    "    )\n",
    "    \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for all Kickstarter pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scraped data\n",
    "scraped_collection = joblib.load('data/2017-09-10_scraped_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(soup, campaign, section):\n",
    "    # Extract all features for the given section, otherwise return np.nan\n",
    "    if campaign[section] == 'section_not_found':\n",
    "        return([np.nan] * 19)\n",
    "    else:\n",
    "        return (\n",
    "            len(get_sentences(campaign[section])),\n",
    "            len(get_words(campaign[section])),\n",
    "            len(identify_allcaps(campaign[section])),\n",
    "            len(identify_allcaps(campaign[section])) / num_words,\n",
    "            count_exclamations(campaign[section]),\n",
    "            count_exclamations(campaign[section]) / num_words,\n",
    "            count_apple_words(campaign[section]),\n",
    "            count_apple_words(campaign[section]) / num_words,\n",
    "            compute_avg_words(campaign[section]),\n",
    "            count_paragraphs(soup, section),\n",
    "            compute_avg_sents_paragraph(soup, section),\n",
    "            compute_avg_words_paragraph(soup, section),\n",
    "            count_images(soup, section),\n",
    "            count_videos(soup, section),\n",
    "            count_youtube(soup, section),\n",
    "            count_gifs(soup, section),\n",
    "            count_hyperlinks(soup, section),\n",
    "            count_bolded(soup, section),\n",
    "            count_bolded(soup, section) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames for each section\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "             'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "             'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "             'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "             'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "             'num_hyperlinks', 'num_bolded', 'percent_bolded']\n",
    "section1_df = pd.DataFrame(columns=features)\n",
    "section2_df = pd.DataFrame(columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in scraped_collection.iterrows():\n",
    "    # Parse scraped HTML\n",
    "    soup = parse(row[0])\n",
    "    \n",
    "    # Extract and normalize campaign sections\n",
    "    campaign = get_campaign(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    campaign['risks'] = normalize(campaign['risks'])\n",
    "    \n",
    "    # Extract features for each section\n",
    "    section1_df.loc[index] = extract_features(soup, campaign, 'about')\n",
    "    section2_df.loc[index] = extract_features(soup, campaign, 'risks')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
