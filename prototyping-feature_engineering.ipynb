{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering for a single Kickstarter project\n",
    "\n",
    "The goal of this notebook is to engineer features that will eventually be extracted from scraped HTML for each project page.\n",
    "\n",
    "## Table of contents\n",
    "1. [Loading scraping and extraction functions](#cell1)\n",
    "2. [Normalizing the campaign sections](#cell2)\n",
    "3. [Defining functions to compute features](#cell3)\n",
    "4. [Extracting all features for a single project](#cell4)\n",
    "\n",
    "<a id=\"cell1\"></a>\n",
    "## 1. Loading scraping and extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the scraping, parsing, and extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(hyperlink):\n",
    "    # Scrape the website\n",
    "    return requests.get(hyperlink)\n",
    "\n",
    "def parse(scraped_html):\n",
    "    # Parse the HTML content\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compress all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    # Extract the 'About this project' section if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Extract the 'Risks and challenges' section if available\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean both sections and return them in a dict\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by selecting a project page and its URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyperlink = 'https://www.kickstarter.com/projects/getpebble/pebble-2-time-2-and-core-an-entirely-new-3g-ultra'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/sbf/sculpto-the-worlds-most-user-friendly-desktop-3d-p?ref=discovery'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/getpebble/pebble-e-paper-watch-for-iphone-and-android'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/1683069409/the-new-york-sorta-marathon?ref=discovery'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/dinobytelabs/midli-a-dark-and-mystical-tale-of-letting-go?ref=category'\n",
    "#hyperlink = 'https://www.kickstarter.com/projects/1385294316/help-me-start-my-cottage-industry-bakesalecom?ref=category_newest'\n",
    "scraped_html = scrape(hyperlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's parse the HTML and extract the campaign sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = parse(scraped_html)\n",
    "campaign = get_campaign(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell2\"></a>\n",
    "## 2. Normalizing the campaign sections\n",
    "\n",
    "Some projects contain sections with email addresses and hyperlinks. Since these terms shouldn't be counted as words, let's tag them so we can avoid them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Tag email addresses\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize campaign sections\n",
    "campaign['about'] = normalize(campaign['about'])\n",
    "campaign['risks'] = normalize(campaign['risks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell3\"></a>\n",
    "## 3. Defining functions to compute features\n",
    "\n",
    "Let's create a function for each feature we want to extract and test the function on the campaign's *About this project* section. If at anytime the campaign section is missing, we'll assign `NaN` to every feature for that section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    # Tokenizes text into sentences and returns them in a list\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the campaign section is missing, assign NaN to the feature's value \n",
    "if campaign['about'] == 'section_not_found':\n",
    "    num_sents = np.nan\n",
    "else:\n",
    "    num_sents = len(get_sentences(campaign['about']))\n",
    "num_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    # Returns the text with punctuation removed\n",
    "    return re.sub(r'[^\\w\\d\\s]', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    # Tokenizes text into words and returns them in a list excluding tags\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text)) \\\n",
    "            if word not in ('emailaddr', 'httpaddr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3549"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    num_words = np.nan\n",
    "else:\n",
    "    num_words = len(get_words(campaign['about']))\n",
    "\n",
    "# If the section contains no words, assign NaN to num_words to avoid potential\n",
    "# division by zero\n",
    "if num_words == 0:\n",
    "    num_words = np.nan \n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of all-caps words and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_allcaps(text):\n",
    "    # Counts the number of all-caps words\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 0.007326007326007326\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        len(identify_allcaps(campaign['about'])),\n",
    "        len(identify_allcaps(campaign['about'])) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of exclamation marks and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_exclamations(text):\n",
    "    # Counts the number of exclamation marks present in the text\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.0033812341504649195\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_exclamations(campaign['about']),\n",
    "        count_exclamations(campaign['about']) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of Apple adjectives and %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_apple_words(text):\n",
    "    # Define a set of Apple adjectives\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    return sum(\n",
    "        1 for word in get_words(text) if word in apple_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.0016906170752324597\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_apple_words(campaign['about']),\n",
    "        count_apple_words(campaign['about']) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average # of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_words(text):\n",
    "    # Compute the mean number of words in each sentence\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in \\\n",
    "         get_sentences(text)]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_words(campaign['about']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the # of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_paragraphs(soup, section):    \n",
    "    # Use tree parsing to compute number of paragraphs\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_paragraphs(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average # of sentences per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "    \n",
    "    # Compute the mean number of sentences in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53383458647\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_sents_paragraph(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average # of words per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_words_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "    \n",
    "    # Compute the mean number of words in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.5112781955\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_words_paragraph(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_images(soup, section):    \n",
    "    # Use tree parsing to compute number of images\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_images(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of embedded videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_videos(soup, section):    \n",
    "    # Use tree parsing to compute number of non-YouTube videos\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('div', class_='video-player'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_videos(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_youtube(soup, section):    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to select all iframe tags\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    # Since YouTube videos are contained in iframe tags, determine which\n",
    "    # iframe tags contain YouTube videos and count them\n",
    "    for iframe in iframes:\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_youtube(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_gifs(soup, section):    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to select all image tags\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    # Since GIFs are contained in image tags, determine which image tags\n",
    "    # contain GIFs and count them\n",
    "    for image in images:\n",
    "        try:\n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_gifs(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_hyperlinks(soup, section):    \n",
    "    # Use tree parsing to compute number of hyperlinks\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_hyperlinks(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of bolded text and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_bolded(soup, section):    \n",
    "    # Use tree parsing to compute number of bolded text\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 0.016060862214708368\n"
     ]
    }
   ],
   "source": [
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_bolded(soup, 'about'),\n",
    "        count_bolded(soup, 'about') / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell4\"></a>\n",
    "## 4. Extracting all features for a single project\n",
    "\n",
    "Let's process all of feature extraction functions on a project page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182, 3549, 26, 0.007326007326007326, 12, 0.0033812341504649195, 6, 0.0016906170752324597, 19.5, 133, 1.5338345864661653, 21.511278195488721, 25, 0, 1, 0, 35, 57, 0.016060862214708368)\n"
     ]
    }
   ],
   "source": [
    "# Extract all features for the given section. If the section isn't available,\n",
    "# then return np.nan for each feature.\n",
    "section = 'about'\n",
    "if campaign[section] == 'section_not_found':\n",
    "    print([np.nan] * 19)\n",
    "else:\n",
    "    row = ( \n",
    "        len(get_sentences(campaign[section])),\n",
    "        len(get_words(campaign[section])),\n",
    "        len(identify_allcaps(campaign[section])),\n",
    "        len(identify_allcaps(campaign[section])) / num_words,\n",
    "        count_exclamations(campaign[section]),\n",
    "        count_exclamations(campaign[section]) / num_words,\n",
    "        count_apple_words(campaign[section]),\n",
    "        count_apple_words(campaign[section]) / num_words,\n",
    "        compute_avg_words(campaign[section]),\n",
    "        count_paragraphs(soup, section),\n",
    "        compute_avg_sents_paragraph(soup, section),\n",
    "        compute_avg_words_paragraph(soup, section),\n",
    "        count_images(soup, section),\n",
    "        count_videos(soup, section),\n",
    "        count_youtube(soup, section),\n",
    "        count_gifs(soup, section),\n",
    "        count_hyperlinks(soup, section),\n",
    "        count_bolded(soup, section),\n",
    "        count_bolded(soup, section) / num_words\n",
    "    )\n",
    "    \n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
