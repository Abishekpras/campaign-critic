{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "**Goal: Load a training set for a campaign section from its PostgreSQL database, train models and save them.** \n",
    "\n",
    "## Table of contents\n",
    "1. [Preparing the training set](#cell1)\n",
    "2. [Training the models](#cell2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import nltk\n",
    "import feature_engineering\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedShuffleSplit, RandomizedSearchCV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by querying the training set from PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set database credentials\n",
    "db_name1 = 'section1_db'\n",
    "usernm = 'redwan'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "pwd = 'pentium'\n",
    "\n",
    "# Prepare a connection to database for section 1\n",
    "con1 = psycopg2.connect(\n",
    "    database=db_name1, \n",
    "    host='localhost',\n",
    "    user=usernm,\n",
    "    password=pwd\n",
    ")\n",
    "\n",
    "# Query all data from both campaign sections\n",
    "sql_query1 = 'SELECT * FROM section1_db;'\n",
    "section1_df_full = pd.read_sql_query(sql_query1, con1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell1\"></a>\n",
    "## 1. Preparing the training set\n",
    "Next, let's build the design matrix for meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of meta features to use in models\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "            'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "            'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "            'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "            'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "            'num_hyperlinks', 'num_bolded', 'percent_bolded']\n",
    "\n",
    "# Select features\n",
    "X = section1_df_full[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all rows with no data\n",
    "X_cleaned = X[~X.isnull().all(axis=1)]\n",
    "\n",
    "# Fill remaining missing values with zero\n",
    "X_cleaned = X_cleaned.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to standardize the meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the meta features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll preprocess the text in the campaign section in preparation for building $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Access stop word dictionary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # Prepare the Porter stemmer\n",
    "    porter = nltk.PorterStemmer()\n",
    "    \n",
    "    # Remove punctuation and lowercase each word\n",
    "    text = feature_engineering.remove_punc(text).lower()\n",
    "    \n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join(\n",
    "        porter.stem(term )\n",
    "        for term in text.split()\n",
    "        if term not in stop_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform preprocessing\n",
    "#preprocessed_text = section1_df_full.loc[X_cleaned.index, 'normalized_text'] \\\n",
    "#    .apply(preprocess_text)\n",
    "    \n",
    "# Alternatively load a pickle that contains already preprocessed text \n",
    "preprocessed_text = joblib.load('data/nlp/preprocessed_text_training_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preprocessed text, let's create an $n$-gram model using unigrams and bigrams using the tf-idf statistic and the top 250 $n$-grams according to term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a design matrix using an n-gram model\n",
    "#vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=250)\n",
    "#X_ngrams = vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# Alternatively we can load a pickle that contains the already constructed \n",
    "# n-grams and a fitted vectorizer\n",
    "X_ngrams = joblib.load('data/nlp/X_ngrams_250.pkl')\n",
    "\n",
    "vectorizer = joblib.load('data/nlp/vectorizer_250.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's combine the meta features with the $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24527, 269)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the meta features into a sparse matrix\n",
    "X_std_sparse = sparse.csr_matrix(X_std)\n",
    "\n",
    "# Concatenate the meta features with the n-grams\n",
    "X_full = sparse.hstack([X_std_sparse, X_ngrams])\n",
    "\n",
    "# Display the shape of the combined matrix for confirmation\n",
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish by collecting the entries of the target variable that correspond to those in the design matrix, and storing them in a separate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the classification target variable\n",
    "y = section1_df_full.loc[X_cleaned.index, 'funded'].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the target variable, whose contents are Booleans, as a numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the class labels in the target variable\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell2\"></a>\n",
    "## 2. Training models\n",
    "\n",
    "We'll use cross-validated grid search to determine the optimal hyperparameters for the desired model, trained on the meta features and $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the recommended number of iterations for SGD\n",
    "SGD_iterations = np.ceil(10 ** 6 / len(X_std))\n",
    "\n",
    "# Initialize the hyperparameter space\n",
    "param_dist = {\n",
    "    'alpha': np.logspace(-6, -1, 50),\n",
    "    'l1_ratio': np.linspace(0, 1, 50)\n",
    "}\n",
    "\n",
    "# Set up a random hyperparameter search and cross-validation strategy\n",
    "random_search_full = RandomizedSearchCV(\n",
    "    estimator=SGDClassifier(\n",
    "        loss='log',\n",
    "        penalty='elasticnet',\n",
    "        max_iter=SGD_iterations,\n",
    "        random_state=41\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',\n",
    "    random_state=41,\n",
    "    n_iter=40,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the random hyperparameter model to identify optimal hyperparameters\n",
    "random_search_full.fit(X_full, y_enc)\n",
    "\n",
    "# Train the classifier on the entire dataset using the optimal hyperparameters\n",
    "clf_full = SGDClassifier(\n",
    "        loss='log',\n",
    "        penalty='elasticnet',\n",
    "        alpha=random_search_full.best_params_['alpha'],\n",
    "        l1_ratio=random_search_full.best_params_['l1_ratio'],\n",
    "        max_iter=SGD_iterations,\n",
    "        random_state=41\n",
    ")\n",
    "clf_full.fit(X_full, y_enc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to use a model that is only trained on the meta features, let's repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a random hyperparameter search and cross-validation strategy for a \n",
    "# model using meta features only\n",
    "random_search_meta = RandomizedSearchCV(\n",
    "    estimator=SGDClassifier(\n",
    "        loss='log',\n",
    "        penalty='elasticnet',\n",
    "        max_iter=SGD_iterations,\n",
    "        random_state=41\n",
    "    ),\n",
    "    param_distributions=param_dist,\n",
    "    cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',\n",
    "    random_state=41,\n",
    "    n_iter=40,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train a random hyperparameter search model on the meta features only\n",
    "random_search_meta.fit(X_std, y_enc)\n",
    "\n",
    "# Train the classifier on the metal features using the optimal hyperparameters\n",
    "clf_meta = SGDClassifier(\n",
    "        loss='log',\n",
    "        penalty='elasticnet',\n",
    "        alpha=random_search_meta.best_params_['alpha'],\n",
    "        l1_ratio=random_search_meta.best_params_['l1_ratio'],\n",
    "        max_iter=SGD_iterations,\n",
    "        random_state=41\n",
    ")\n",
    "clf_meta.fit(X_std, y_enc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the classifiers, in addition, to the scaler and vectorizer used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the classifiers, the vectorizer and scaler\n",
    "#joblib.dump(clf_full, 'trained_classifier.pkl')\n",
    "#joblib.dump(clf_meta, 'trained_classifier_meta_only.pkl')\n",
    "#joblib.dump(scaler, 'trained_scaler.pkl')\n",
    "#joblib.dump(vectorizer, 'vectorizer_250.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
