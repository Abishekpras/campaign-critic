{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Kickstarter project pages\n",
    "\n",
    "**Goal: Load parsed data from Web Robots database, containing URLs and other aspects of Kickstarter projects, scrape each project page, and save the results in a table.**\n",
    "\n",
    "Note: 2000 scraped pages take up ~1 GB of storage. In addition, the scraping rate used in this notebook converges to about 0.28 pages/second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import time\n",
    "import random\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the table containing already-parsed Web Robots data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load table containing Web Robots data\n",
    "df = joblib.load('data/web_robots_data/web_robots_data_to_06-2017.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed data includes:\n",
    "\n",
    "- `name` - project's name\n",
    "- `category` - project's category as defined by Kickstarter\n",
    "- `hyperlink` - project's web page URL\n",
    "- `currency` - type of currency used for fundraising\n",
    "- `pledged` - total amount of money pledged by backers over the course of the project\n",
    "- `goal` - funding goal set by the creator\n",
    "- `location` - creator's location information\n",
    "\n",
    "Let's select the projects that only use U.S. dollars to ensure we're working with projects written in American English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select projects described in American English\n",
    "df_USD = df[df['currency'] == 'USD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Web Robots data contains nearly 200,000 projects, we'll select a random sample for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a random sample of the Web Robots data using a seed value to ensure\n",
    "# repeatability\n",
    "seed = np.random.seed(42)\n",
    "df_sample = df_USD.sample(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the scraping process, we'll monitor the overall progress and measure how fast we're scraping to avoid overloading the Kickstarter server. Afterwards, we'll report the total run time, average scraping speed and the total number of scraped project pages. We'll also keep track of the position of the last scraped project page, in case the scraper halts for any reason, to note where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: 1; Row ID: 190378; Frequency: 0.31547747899263323 requests/sec\n",
      "\n",
      "Run time: 6.344876289367676\n",
      "Average rate: 0.3152149717010981\n",
      "# of projects scraped: 2\n"
     ]
    }
   ],
   "source": [
    "# Initalize an empty DataFrame to store scraped HTML\n",
    "scraped_collection = pd.DataFrame(columns=['scraped_HTML'])\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the number of requests\n",
    "request_count = 0\n",
    "\n",
    "# Select which projects to scrape via its index. This is used for starting\n",
    "# at a position other than the beginning in case the scraper stopped \n",
    "# unexpectedly.\n",
    "starting_point = 0\n",
    "ending_point = 24000\n",
    "\n",
    "for index, row in df_sample[starting_point:ending_point].iterrows():\n",
    "    # Perform a request and timeout after 20 seconds since some pages may take\n",
    "    # longer to scrape\n",
    "    scraped_html = requests.get(row['hyperlink'], timeout=20)\n",
    "    \n",
    "    # Pause the loop for a random amount of time\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    \n",
    "    # Monitor the requests by clearing the output and displaying current \n",
    "    # progress\n",
    "    elapsed_time = time.time() - start_time\n",
    "    clear_output(wait = True)\n",
    "    print(\n",
    "        'Request: {}; Row ID: {}; Frequency: {} requests/sec'.format(\n",
    "            request_count + starting_point,\n",
    "            index,\n",
    "            (request_count + 1) / elapsed_time\n",
    "        )\n",
    "    )\n",
    "    request_count += 1\n",
    "    \n",
    "    # Record scraped HTML\n",
    "    scraped_collection.loc[index, 'scraped_HTML'] = scraped_html\n",
    "    \n",
    "# Display the overall time, average scraping speed and total number of scraped\n",
    "# project pages\n",
    "run_time = time.time() - start_time\n",
    "print()\n",
    "print('Run time:', run_time)\n",
    "print('Average rate:', len(scraped_collection) / run_time)\n",
    "print('# of projects scraped:', len(scraped_collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the collection of scraped HTML and label the filename with the indices of the projects scraped to keep track off how far into the random sample we've scraped. This way, it's easy restart the scraper where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scraped_collection_0-1.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the data table containing the scraped HTML for each project\n",
    "joblib.dump(\n",
    "    scraped_collection, 'scraped_collection_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point - 1\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
