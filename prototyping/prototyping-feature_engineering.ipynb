{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering meta features for a Kickstarter project\n",
    "\n",
    "**Goal: Develop a feature engineering strategy for meta features from content scraped from a Kickstarter project page.**\n",
    "\n",
    "## Table of contents\n",
    "1. [Loading scraping and extraction functions](#cell1)\n",
    "2. [Normalizing the campaign sections](#cell2)\n",
    "3. [Defining functions to compute meta features](#cell3)\n",
    "4. [Extracting all meta features for a project](#cell4)\n",
    "\n",
    "<a id=\"cell1\"></a>\n",
    "## 1. Loading scraping and extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the scraping, parsing, and extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(hyperlink):\n",
    "    # Scrape the website\n",
    "    return requests.get(hyperlink)\n",
    "\n",
    "def parse(scraped_html):\n",
    "    # Parse the HTML content using an lxml parser\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compress all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    # Collect the \"About this project\" section if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Collect the \"Risks and challenges\" section if available, and remove all\n",
    "    # unnecessary text\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean both sections and return them in a dictionary\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by selecting a project page and its URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a test hyperlink\n",
    "hyperlink = 'https://www.kickstarter.com/projects/1385294316/help-me-start' + \\\n",
    "    '-my-cottage-industry-bakesalecom?ref=category_newest'\n",
    "scraped_html = scrape(hyperlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's parse the HTML and extract the campaign sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the scraped HTML and collect the campaign sections\n",
    "soup = parse(scraped_html)\n",
    "campaign = get_campaign(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell2\"></a>\n",
    "## 2. Normalizing the campaign sections\n",
    "\n",
    "Some projects contain email addresses, phone numbers, URLs, money amounts, percentages, or plain numbers. Let's replace them with a tag so they aren't identified as unique words and don't inflate the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Tag email addresses\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag money amounts\n",
    "    normalized = re.sub(r'\\$\\d+(\\.\\d+)?', 'dollramt', normalized)\n",
    "    \n",
    "    # Tag percentages\n",
    "    normalized = re.sub(r'\\d+(\\.\\d+)?\\%', 'percntg', normalized)\n",
    "    \n",
    "    # Tag phone numbers\n",
    "    normalized = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag plain numbers\n",
    "    return re.sub(r'\\d+(\\.\\d+)?', 'numbr', normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's normalize each campaign section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize campaign sections\n",
    "campaign['about'] = normalize(campaign['about'])\n",
    "campaign['risks'] = normalize(campaign['risks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell3\"></a>\n",
    "## 3. Defining functions to compute meta features\n",
    "\n",
    "Let's define a function for each meta feature we want to extract and test the function on the campaign's \"About this project\" section. If at anytime the campaign section is missing, we'll assign `NaN` to every feature for that particular section to note it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    # Tokenize text into sentences and return them in a list\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the campaign section is missing, assign NaN to 'num_sents', otherwise\n",
    "# count and display the number of sentences\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    num_sents = np.nan\n",
    "else:\n",
    "    num_sents = len(get_sentences(campaign['about']))\n",
    "num_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    # Return text with punctuation removed\n",
    "    return re.sub(r'[^\\w\\d\\s]|\\_', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    # Tokenize text into words and return them in a list\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the campaign section is missing, assign NaN to 'num_words', otherwise\n",
    "# count and display the number of words\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    num_words = np.nan\n",
    "else:\n",
    "    num_words = len(get_words(campaign['about']))\n",
    "\n",
    "# If the section contains no words, assign NaN to num_words to catch potential\n",
    "# division by zero errors\n",
    "if num_words == 0:\n",
    "    num_words = np.nan \n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of all-caps words and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_allcaps(text):\n",
    "    # Count the number of all-caps words\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of all-caps\n",
    "# words and its percentage\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        len(identify_allcaps(campaign['about'])),\n",
    "        len(identify_allcaps(campaign['about'])) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of exclamation marks and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_exclamations(text):\n",
    "    # Count the number of exclamation marks in the text\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.013452914798206279\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of ! marks\n",
    "# and its percentage\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_exclamations(campaign['about']),\n",
    "        count_exclamations(campaign['about']) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of Apple adjectives and %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_apple_words(text):\n",
    "    # Define a set of Apple adjectives\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    # Count total number of Apple adjectives in the text\n",
    "    return sum(1 for word in get_words(text) if word in apple_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of Apple\n",
    "# adjectives and its percentage\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_apple_words(campaign['about']),\n",
    "        count_apple_words(campaign['about']) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average # of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_words(text):\n",
    "    # Compute the average number of words in each sentence\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in \\\n",
    "         get_sentences(text)]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.8666666667\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the average # of\n",
    "# words per sentence\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_words(campaign['about']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the # of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_paragraphs(soup, section):    \n",
    "    # Use tree parsing to compute number of paragraphs\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of\n",
    "# paragraphs\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_paragraphs(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average # of sentences per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of sentences in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.88888888889\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the average # of\n",
    "# sentences per paragraph\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_sents_paragraph(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the average # of words per paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_avg_words_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of words in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7777777778\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the average # of\n",
    "# words per paragraph\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(compute_avg_words_paragraph(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_images(soup, section):    \n",
    "    # Use tree parsing to compute number of images\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of images\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_images(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of embedded videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_videos(soup, section):    \n",
    "    # Use tree parsing to compute number of non-YouTube videos\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('div', class_='video-player'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of\n",
    "# non-YouTube videos\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_videos(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_youtube(soup, section):    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to select all iframe tags\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    # Since YouTube videos are contained only in iframe tags, determine which\n",
    "    # iframe tags contain YouTube videos and count them\n",
    "    for iframe in iframes:\n",
    "        # Catch any iframes that fail to include a YouTube source link\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of\n",
    "# YouTube videos\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_youtube(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_gifs(soup, section):    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to select all image tags\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    # Since GIFs are contained in image tags, determine which image tags\n",
    "    # contain GIFs and count them\n",
    "    for image in images:\n",
    "        # Catch any iframes that fail to include an image source link\n",
    "        try:\n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of\n",
    "# GIFs\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_gifs(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_hyperlinks(soup, section):    \n",
    "    # Use tree parsing to compute number of hyperlinks\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of\n",
    "# hyperlinks\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan)\n",
    "else:\n",
    "    print(count_hyperlinks(soup, 'about'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of bolded text and compute %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_bolded(soup, section):    \n",
    "    # Use tree parsing to compute number of bolded text tags\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Display 'NaN' if the section isn't found, otherwise display the # of\n",
    "# bolded text tags and its percentage\n",
    "if campaign['about'] == 'section_not_found':\n",
    "    print(np.nan, np.nan)\n",
    "else:\n",
    "    print(\n",
    "        count_bolded(soup, 'about'),\n",
    "        count_bolded(soup, 'about') / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell4\"></a>\n",
    "## 4. Extracting all meta features for a project\n",
    "\n",
    "Let's process all of the meta feature extraction functions on a project page and return a feature vector for that project that includes both the meta features and normalized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                    15\n",
      "1                                                   223\n",
      "2                                                     0\n",
      "3                                                     0\n",
      "4                                                     3\n",
      "5                                             0.0134529\n",
      "6                                                     0\n",
      "7                                                     0\n",
      "8                                               14.8667\n",
      "9                                                     9\n",
      "10                                              1.88889\n",
      "11                                              24.7778\n",
      "12                                                    0\n",
      "13                                                    0\n",
      "14                                                    0\n",
      "15                                                    0\n",
      "16                                                    0\n",
      "17                                                    0\n",
      "18                                                    0\n",
      "19    I am a numbr year old woman who was a general ...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract all features for the given section. If the section isn't \n",
    "# available, then return 'NaN' for each feature.\n",
    "section = 'about'\n",
    "if campaign[section] == 'section_not_found':\n",
    "    print([np.nan] * 20)\n",
    "else:\n",
    "    row = ( \n",
    "        len(get_sentences(campaign[section])),\n",
    "        len(get_words(campaign[section])),\n",
    "        len(identify_allcaps(campaign[section])),\n",
    "        len(identify_allcaps(campaign[section])) / num_words,\n",
    "        count_exclamations(campaign[section]),\n",
    "        count_exclamations(campaign[section]) / num_words,\n",
    "        count_apple_words(campaign[section]),\n",
    "        count_apple_words(campaign[section]) / num_words,\n",
    "        compute_avg_words(campaign[section]),\n",
    "        count_paragraphs(soup, section),\n",
    "        compute_avg_sents_paragraph(soup, section),\n",
    "        compute_avg_words_paragraph(soup, section),\n",
    "        count_images(soup, section),\n",
    "        count_videos(soup, section),\n",
    "        count_youtube(soup, section),\n",
    "        count_gifs(soup, section),\n",
    "        count_hyperlinks(soup, section),\n",
    "        count_bolded(soup, section),\n",
    "        count_bolded(soup, section) / num_words,\n",
    "        campaign[section]\n",
    "    )\n",
    "    \n",
    "    print(pd.Series(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
