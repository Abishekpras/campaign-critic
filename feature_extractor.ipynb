{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor\n",
    "\n",
    "The goal of this script is to load a DataFrame containing scraped HTML, parse it, and extract features for both sections of the Kickstarter project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load scraping, parsing, and campaign extraction and cleaning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(hyperlink):\n",
    "    # Scrape the website\n",
    "    return requests.get(hyperlink)\n",
    "\n",
    "def parse(scraped_html):\n",
    "    # Parse the HTML\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compresses all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    # Extract the 'About this project' section, if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Extract the 'Risks and challenges' section, if available\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean up both sections and return them in a dict\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}\n",
    "\n",
    "def normalize(text):\n",
    "    # Tag email addresses\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all feature extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    # Tokenizes text into sentences and returns them in a list\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    # Returns the text with punctuation removed\n",
    "    return re.sub(r'[^\\w\\d\\s]', ' ', text)\n",
    "\n",
    "def get_words(text):\n",
    "    # Tokenizes text into words and returns them in a list, excluding tags\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text)) \\\n",
    "            if word not in ('emailaddr', 'httpaddr')]\n",
    "\n",
    "def identify_allcaps(text):\n",
    "    # Counts the number of all-caps words\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)\n",
    "\n",
    "def count_exclamations(text):\n",
    "    # Counts the number of exclamation marks present in the text\n",
    "    return text.count('!')\n",
    "\n",
    "def count_apple_words(text):\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    return sum(\n",
    "        1 for word in get_words(text) if word in apple_words\n",
    "    )\n",
    "\n",
    "def compute_avg_words(text):\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in \\\n",
    "         get_sentences(text)]\n",
    "    ).mean()\n",
    "\n",
    "def count_paragraphs(soup, section):    \n",
    "    # Use tree parsing to compute # of paragraphs for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p'))\n",
    "    \n",
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "        \n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def compute_avg_words_paragraph(soup, section):\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def count_images(soup, section):    \n",
    "    # Use tree parsing to compute # of images for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('img'))\n",
    "    \n",
    "def count_videos(soup, section):    \n",
    "    # Use tree parsing to compute # of images for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('div', class_='video-player'))\n",
    "\n",
    "def count_youtube(soup, section):    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to compute # of YouTube videos for each section\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    for iframe in iframes:\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count\n",
    "\n",
    "def count_gifs(soup, section):    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to compute # of GIFs for each section\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    for image in images:\n",
    "        try:\n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count\n",
    "\n",
    "def count_hyperlinks(soup, section):    \n",
    "    # Use tree parsing to compute hyperlinks for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('a'))\n",
    "    \n",
    "def count_bolded(soup, section):    \n",
    "    # Use tree parsing to compute hyperlinks for each section\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DataFrame containing scraped HTML for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load scraped data\n",
    "file_name = '/mnt/c/Users/Redwan Huq/Downloads/scraped_collection_0-3158.pkl'\n",
    "scraped_collection = joblib.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a master function to extract all features for a project and a specific `section`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(soup, campaign, section):\n",
    "    num_words = len(get_words(campaign[section]))\n",
    "    \n",
    "    # Extract all features for the given section, otherwise return np.nan\n",
    "    if campaign[section] == 'section_not_found':\n",
    "        return([np.nan] * 19)\n",
    "    else:\n",
    "        return (\n",
    "            len(get_sentences(campaign[section])),\n",
    "            num_words,\n",
    "            len(identify_allcaps(campaign[section])),\n",
    "            len(identify_allcaps(campaign[section])) / num_words,\n",
    "            count_exclamations(campaign[section]),\n",
    "            count_exclamations(campaign[section]) / num_words,\n",
    "            count_apple_words(campaign[section]),\n",
    "            count_apple_words(campaign[section]) / num_words,\n",
    "            compute_avg_words(campaign[section]),\n",
    "            count_paragraphs(soup, section),\n",
    "            compute_avg_sents_paragraph(soup, section),\n",
    "            compute_avg_words_paragraph(soup, section),\n",
    "            count_images(soup, section),\n",
    "            count_videos(soup, section),\n",
    "            count_youtube(soup, section),\n",
    "            count_gifs(soup, section),\n",
    "            count_hyperlinks(soup, section),\n",
    "            count_bolded(soup, section),\n",
    "            count_bolded(soup, section) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize empty DataFrames for each section containing column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "             'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "             'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "             'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "             'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "             'num_hyperlinks', 'num_bolded', 'percent_bolded']\n",
    "section1_df = pd.DataFrame(columns=features)\n",
    "section2_df = pd.DataFrame(columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse through each project using the scraped HTML and extract features for both sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in scraped_collection[0:4].iterrows():\n",
    "    # Parse scraped HTML\n",
    "    soup = parse(row[0])\n",
    "    \n",
    "    # Extract and normalize campaign sections\n",
    "    campaign = get_campaign(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    campaign['risks'] = normalize(campaign['risks'])\n",
    "    \n",
    "    # Extract features for each section\n",
    "    section1_df.loc[index] = extract_features(soup, campaign, 'about')\n",
    "    section2_df.loc[index] = extract_features(soup, campaign, 'risks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining extracted features with features collected from the Web Robots database\n",
    "\n",
    "Since the Web Robots data contains the target variable and other interesting features, we'll need to join these data with the DataFrames containing the newly extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Web Robots data\n",
    "web_robots_data = joblib.load(\n",
    "    'data/web_robots_data/web_robots_data_to_06-2017.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the index labels as a new column called `index` to serve as the join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "web_robots_data = web_robots_data.reset_index()\n",
    "section1_df = section1_df.reset_index()\n",
    "section2_df = section2_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the join for each campaign section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section1_merged = section1_df.merge(web_robots_data, how='left', on='index')\n",
    "section2_merged = section2_df.merge(web_robots_data, how='left', on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the merged DataFrames and label the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the start and end projects\n",
    "starting_point = int(file_name.partition('collection_')[2].partition('-')[0])\n",
    "ending_point = int(file_name.partition('-')[2].partition('.pkl')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section2_extracted_data_0-3157.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(section1_merged, 'section1_extracted_data_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point - 1\n",
    "    )\n",
    ")\n",
    "\n",
    "joblib.dump(section2_merged, 'section2_extracted_data_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point - 1\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
