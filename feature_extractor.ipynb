{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor\n",
    "\n",
    "The goal of this script is to load a DataFrame containing scraped HTML, parse it, and extract features for both sections of the Kickstarter project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the scraping, parsing, and campaign extraction and cleaning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(hyperlink):\n",
    "    # Scrape the website\n",
    "    return requests.get(hyperlink)\n",
    "\n",
    "def parse(scraped_html):\n",
    "    # Parse the HTML content\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "def clean_up(messy_text):    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compress all\n",
    "    # whitespace to a single space\n",
    "    clean_text = ' '.join(messy_text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return clean_text.replace(\n",
    "        \"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",\n",
    "        ''\n",
    "    )\n",
    "\n",
    "def get_campaign(soup):\n",
    "    # Collect the 'About this project' section if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Collect the 'Risks and challenges' section if available\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges', '') \\\n",
    "            .replace('Learn about accountability on Kickstarter', '')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean both sections and return them in a dict\n",
    "    return {'about': clean_up(section1), 'risks': clean_up(section2)}\n",
    "\n",
    "def normalize(text):\n",
    "    # Tag email addresses\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the feature extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    # Tokenizes text into sentences and returns them in a list\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    # Returns the text with punctuation removed\n",
    "    return re.sub(r'[^\\w\\d\\s]', ' ', text)\n",
    "\n",
    "def get_words(text):\n",
    "    # Tokenizes text into words and returns them in a list, excluding tags\n",
    "    return [word for word in nltk.word_tokenize(remove_punc(text)) \\\n",
    "            if word not in ('emailaddr', 'httpaddr')]\n",
    "\n",
    "def identify_allcaps(text):\n",
    "    # Counts the number of all-caps words\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)\n",
    "\n",
    "def count_exclamations(text):\n",
    "    # Counts the number of exclamation marks present in the text\n",
    "    return text.count('!')\n",
    "\n",
    "def count_apple_words(text):\n",
    "    # Define a set of Apple adjectives\n",
    "    apple_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    return sum(\n",
    "        1 for word in get_words(text) if word in apple_words\n",
    "    )\n",
    "\n",
    "def compute_avg_words(text):\n",
    "    # Compute the mean number of words in each sentence\n",
    "    return pd.Series(\n",
    "        [len(get_words(sentence)) for sentence in \\\n",
    "         get_sentences(text)]\n",
    "    ).mean()\n",
    "\n",
    "def count_paragraphs(soup, section):    \n",
    "    # Use tree parsing to compute number of paragraphs\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p'))\n",
    "    \n",
    "def compute_avg_sents_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "    \n",
    "    # Compute the mean number of sentences in each paragraph    \n",
    "    return pd.Series(\n",
    "        [len(get_sentences(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def compute_avg_words_paragraph(soup, section):\n",
    "    # Use tree parsing to identify all paragraphs\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('p')\n",
    "    \n",
    "    # Compute the mean number of words in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(get_words(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def count_images(soup, section):    \n",
    "    # Use tree parsing to compute number of images\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('img'))\n",
    "    \n",
    "def count_videos(soup, section):    \n",
    "    # Use tree parsing to compute number of non-YouTube videos\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('div', class_='video-player'))\n",
    "\n",
    "def count_youtube(soup, section):    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to select all iframe tags\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    # Since YouTube videos are contained in iframe tags, determine which\n",
    "    # iframe tags contain YouTube videos and count them\n",
    "    for iframe in iframes:\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count\n",
    "\n",
    "def count_gifs(soup, section):    \n",
    "    # Initialize total number of GIFs\n",
    "    gif_count = 0\n",
    "\n",
    "    # Use tree parsing to select all image tags\n",
    "    if section == 'about':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('img')\n",
    "    elif section == 'risks':\n",
    "        images = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img')\n",
    "    \n",
    "    # Since GIFs are contained in image tags, determine which image tags\n",
    "    # contain GIFs and count them\n",
    "    for image in images:\n",
    "        try:\n",
    "            if 'gif' in image.get('data-src'):\n",
    "                gif_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return gif_count\n",
    "\n",
    "def count_hyperlinks(soup, section):    \n",
    "    #Use tree parsing to compute number of hyperlinks\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('a'))\n",
    "    \n",
    "def count_bolded(soup, section):    \n",
    "    # Use tree parsing to compute number of bolded text\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "            ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "            ).find_all('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the database containing scraped HTML for each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load scraped data\n",
    "filename = '/mnt/c/Users/Redwan Huq/Downloads/scraped_collection_3159-5614.pkl'\n",
    "scraped_collection = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a master function that extracts all features for the campaign section of project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(soup, campaign, section):\n",
    "    # Compute the number of words in the section\n",
    "    num_words = len(get_words(campaign[section]))\n",
    "    \n",
    "    # If the section contains no words, assign NaN to num_words to avoid\n",
    "    # potential division by zero\n",
    "    if num_words == 0:\n",
    "        num_words = np.nan\n",
    "        \n",
    "    # Extract all features for the given section. If the section isn't \n",
    "    # available, then return np.nan for each feature.\n",
    "    if campaign[section] == 'section_not_found':\n",
    "        return([np.nan] * 19)\n",
    "    else:\n",
    "        return (\n",
    "            len(get_sentences(campaign[section])),\n",
    "            num_words,\n",
    "            len(identify_allcaps(campaign[section])),\n",
    "            len(identify_allcaps(campaign[section])) / num_words,\n",
    "            count_exclamations(campaign[section]),\n",
    "            count_exclamations(campaign[section]) / num_words,\n",
    "            count_apple_words(campaign[section]),\n",
    "            count_apple_words(campaign[section]) / num_words,\n",
    "            compute_avg_words(campaign[section]),\n",
    "            count_paragraphs(soup, section),\n",
    "            compute_avg_sents_paragraph(soup, section),\n",
    "            compute_avg_words_paragraph(soup, section),\n",
    "            count_images(soup, section),\n",
    "            count_videos(soup, section),\n",
    "            count_youtube(soup, section),\n",
    "            count_gifs(soup, section),\n",
    "            count_hyperlinks(soup, section),\n",
    "            count_bolded(soup, section),\n",
    "            count_bolded(soup, section) / num_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize empty DataFrames for each campaign section and set the column names with feature labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "             'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "             'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "             'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "             'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "             'num_hyperlinks', 'num_bolded', 'percent_bolded']\n",
    "section1_df = pd.DataFrame(columns=features)\n",
    "section2_df = pd.DataFrame(columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's a) parse through the scraped HTML of each project, b) collect the campaign, c) normalize both campaign sections, and d) extract features for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, row in scraped_collection.iterrows():\n",
    "    # Parse scraped HTML\n",
    "    soup = parse(row[0])\n",
    "    \n",
    "    # Extract and normalize campaign sections\n",
    "    campaign = get_campaign(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    campaign['risks'] = normalize(campaign['risks'])\n",
    "    \n",
    "    # Extract features for each section\n",
    "    section1_df.loc[index] = extract_features(soup, campaign, 'about')\n",
    "    section2_df.loc[index] = extract_features(soup, campaign, 'risks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining extracted features with Web Robots data\n",
    "\n",
    "Since the Web Robots data contains the target variable, as well as other interesting features such as `category`, let's join these two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Web Robots data\n",
    "web_robots_data = joblib.load(\n",
    "    'data/web_robots_data/web_robots_data_to_06-2017.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a shared key to complete the join. Let's turn the index labels into a new column called `index`&mdash;it can serve as the join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn the index labels into a new column\n",
    "web_robots_data = web_robots_data.reset_index()\n",
    "section1_df = section1_df.reset_index()\n",
    "section2_df = section2_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join the extracted features with the Web Robots data for each campaign section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join Web Robots data with the extracted features using only the projects\n",
    "# that have been processed\n",
    "section1_merged = section1_df.merge(web_robots_data, how='left', on='index')\n",
    "section2_merged = section2_df.merge(web_robots_data, how='left', on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the joined data, let's extract the starting and ending points to label the filename with the data it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the start and end projects using the filename of the database\n",
    "# containing the scraped HTML collection\n",
    "starting_point = int(filename.partition('collection_')[2].partition('-')[0])\n",
    "ending_point = int(filename.partition('-')[2].partition('.pkl')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the final data table for each section and label their filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section2_extracted_data_3159-5614.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the data table for each section\n",
    "joblib.dump(section1_merged, 'section1_extracted_data_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point\n",
    "    )\n",
    ")\n",
    "\n",
    "joblib.dump(section2_merged, 'section2_extracted_data_{}-{}.pkl'.format(\n",
    "        starting_point,\n",
    "        ending_point\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
