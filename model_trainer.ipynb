{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "**The purpose of this notebook is to load a training set for a campaign section from its PostgreSQL database, train validated model(s), and store the model(s).** \n",
    "\n",
    "## Table of contents\n",
    "1. [Preparing the training set](#cell1)\n",
    "2. [Training the models](#cell2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import nltk\n",
    "import feature_engineering\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (StratifiedShuffleSplit, GridSearchCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by querying the training set from PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set database credentials\n",
    "db_name1 = 'section1_db'\n",
    "usernm = 'redwan'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "#pwd = ''\n",
    "\n",
    "# Prepare a connection to database for section 1\n",
    "con1 = psycopg2.connect(\n",
    "    database=db_name1, \n",
    "    host='localhost',\n",
    "    user=usernm,\n",
    "    password=pwd\n",
    ")\n",
    "\n",
    "# Query all data from both campaign sections\n",
    "sql_query1 = 'SELECT * FROM section1_db;'\n",
    "section1_df_full = pd.read_sql_query(sql_query1, con1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell1\"></a>\n",
    "## 1. Preparing the training set\n",
    "Next, let's build the design matrix for meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of meta features to use in models\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "            'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "            'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "            'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "            'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "            'num_hyperlinks', 'num_bolded', 'percent_bolded']\n",
    "\n",
    "# Select features\n",
    "X = section1_df_full[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all rows with no data\n",
    "X_cleaned = X[~X.isnull().all(axis=1)]\n",
    "\n",
    "# Fill remaining missing values with zero\n",
    "X_cleaned = X_cleaned.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to standardize the meta features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the meta features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll preprocess the text in the campaign section in preparation for building $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Access stop word dictionary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # Prepare the Porter stemmer\n",
    "    porter = nltk.PorterStemmer()\n",
    "    \n",
    "    # Remove punctuation and lowercase each word\n",
    "    text = feature_engineering.remove_punc(text).lower()\n",
    "    \n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join(\n",
    "        porter.stem(term )\n",
    "        for term in text.split()\n",
    "        if term not in stop_words\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "#preprocessed_text = section1_df_full.loc[X_cleaned.index, 'normalized_text'] \\\n",
    "#    .apply(preprocess_text)\n",
    "    \n",
    "# Alternatively load a pickle that contains the already preprocessed text \n",
    "preprocessed_text = joblib.load(\n",
    "    'data/processed_text-n_grams/preprocessed_text_training_set.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the preprocessed text, let's create an $n$-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a design matrix using an n-gram model and tf-idf statistics\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=250)\n",
    "X_ngrams = vectorizer.fit_transform(preprocessed_text)\n",
    "\n",
    "# Alternatively we can load a pickle that contains the already constructed \n",
    "# n-grams and a fitted vectorizer\n",
    "#X_ngrams = joblib.load(\n",
    "#    'data/processed_text-n_grams/X_ngrams_250.pkl'\n",
    "#)\n",
    "\n",
    "#vectorizer = joblib.load(\n",
    "#    'data/processed_text-n_grams/vectorizer_250.pkl'\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's combine the meta features with the $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24527, 269)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the meta features into a sparse matrix\n",
    "X_std_sparse = sparse.csr_matrix(X_std)\n",
    "\n",
    "# Concatenate the meta features with the n-grams\n",
    "X_full = sparse.hstack([X_std_sparse, X_ngrams])\n",
    "\n",
    "# Display the shape of the combined matrix for confirmation\n",
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish up by collecting the entries for the target variable that correspond to those in the design matrix, and storing them in a separate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the classification target variable\n",
    "y = section1_df_full.loc[X_cleaned.index, 'funded'].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the target variable, whose contents are Booleans, as a numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the class labels in the target variable\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell2\"></a>\n",
    "## 2. Training models\n",
    "\n",
    "We'll use cross-validated grid search to determine the optimal hyperparameters for the desired model, trained on the meta features and $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a range of values to test the regularization hyperparameter\n",
    "param_grid = [{'C': np.logspace(-3, 3, 10)}]\n",
    "\n",
    "# Set up a grid search and cross-validation strategy\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train a grid search model on meta features and n-grams\n",
    "grid_search.fit(X_full, y_enc)\n",
    "\n",
    "# Train the classifier on the entire dataset using the optimal hyperparameter\n",
    "final_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "final_clf.fit(X_full, y_enc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the model training process on the training set but exclude $n$-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a grid search and cross-validation strategy\n",
    "grid_search2 = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train a grid search model on the meta features only\n",
    "grid_search2.fit(X_std, y_enc)\n",
    "\n",
    "# Train the classifier on the entire dataset using the optimal hyperparameter\n",
    "final_clf2 = LogisticRegression(C=grid_search2.best_params_['C'])\n",
    "final_clf2.fit(X_std, y_enc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the classifiers, in addition, to the scaler and vectorizer used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Serialize the classifiers and the scaler objects\n",
    "#joblib.dump(final_clf, 'trained_classifier.pkl')\n",
    "#joblib.dump(final_clf2, 'trained_classifier_meta_only.pkl')\n",
    "#joblib.dump(scaler, 'trained_scaler.pkl')\n",
    "#joblib.dump(vectorizer, 'vectorizer_250.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
