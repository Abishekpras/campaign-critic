{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "**The purpose of this notebook is to load a training set for a campaign section from its PostgreSQL database, train a model, and store the model for future use.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "import feature_engineering\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, learning_curve, StratifiedShuffleSplit, GridSearchCV,\n",
    "    cross_val_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by querying the training set from PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set database credentials\n",
    "db_name = 'section1'\n",
    "usernm = 'redwan'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "# pwd = ''\n",
    "\n",
    "# Prepare a connection to a database for a campaign section\n",
    "con = psycopg2.connect(\n",
    "    database=db_name, \n",
    "    host='localhost',\n",
    "    user=usernm,\n",
    "    password=pwd\n",
    ")\n",
    "\n",
    "# Query all data from a campaign section\n",
    "sql_query = 'SELECT * FROM {}'.format(db_name)\n",
    "section_df_full = pd.read_sql_query(sql_query, con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's build the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A list of features to use in the model\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "            'num_exclms', 'percent_exclms', 'num_apple_words',\n",
    "            'percent_apple_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "            'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "            'num_images', 'num_videos', 'num_youtubes', 'num_gifs',\n",
    "            'num_hyperlinks', 'num_bolded', 'percent_bolded']\n",
    "\n",
    "# Select features\n",
    "X = section_df_full[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all rows with no data\n",
    "X_cleaned = X[~X.isnull().all(axis=1)]\n",
    "\n",
    "# Fill remaining missing values with zero\n",
    "X_cleaned = X_cleaned.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's collect the entries for the target variable that correspond to those in the design matrix, and store them in a separate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select rows of the target variable corresponding to the cleaned design matrix\n",
    "y = section_df_full.loc[X_cleaned.index, 'funded'].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the target variable, whose contents are Booleans, as a numeric variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode the class labels in the target variable\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use grid search and cross-validation to determine the optimal hyperparameters for the desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.0046415888336127772}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a range of values for testing the hyperparameters\n",
    "param_grid = [{'C': np.logspace(-3, 3, 10)}]\n",
    "\n",
    "# Set up a grid search and cross-validation strategy\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=param_grid,\n",
    "    cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train a grid search model to identify optimal hyperparameters\n",
    "grid_search.fit(X_std, y_enc)\n",
    "\n",
    "# Display the optimal hyperparameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the optimal hyperparameters identified by the grid search and train a final model on the complete training set. This model is ready to be deployed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the classifier on the entire dataset using the optimal hyperparameter\n",
    "final_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "final_clf.fit(X_std, y_enc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the classifier, in addition, to the scaler object used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Serialize the classifier and the scaler objects\n",
    "joblib.dump(final_clf, 'trained_classifier.pkl')\n",
    "joblib.dump(scaler, 'trained_scaler.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
